# GenAI World Model Testing: Vision Perception Validation

A research project testing whether GenAI-generated videos from Nvidia Cosmos Predict 2.5 can be used for robotics perception algorithm validation.

## Project Overview

This project implements a vision-based line-following controller for the e-puck robot in Webots. Instead of using live camera feeds, the controller reads pre-recorded videos generated by Cosmos Predict 2.5. This tests whether synthetic videos can simulate real-world perception scenarios for robotics testing.

## Architecture

### Core Components

**ResearchRobot Class** (`vision_controller.py`)

- Extends Webots `Robot` controller
- Control loop runs at simulation timestep (32ms)
- Flow: Video Frame → HSV Conversion → Yellow Line Detection → Steering Command → Motor Control → Data Logging

**Perception Pipeline**

- Reads pre-recorded MP4 videos
- HSV color space yellow line detection (range: [15,100,100] to [35,255,255])
- Centroid calculation using image moments
- Steering error computed as normalized horizontal offset from center

**Control System**

- Differential drive controller with proportional steering (gain: 2.5)
- Base speed: 3.0 rad/s
- Motor commands: `v_left = base_speed + turn_cmd`, `v_right = base_speed - turn_cmd`

**Data Logging**

- CSV format: `Time, Frame, Error, Motor_L, Motor_R, Odom_L, Odom_R`
- Odometry calculated from wheel encoders (wheel radius: 0.02m)

## Project Structure

```text
vision_controller/
├── vision_controller.py           # Main Webots controller
├── worlds/
│   └── e-puck.wbt                 # Webots world file
├── videos/                        # Cosmos-generated videos
│   ├── path_asphalt.mp4          # Baseline scenario
│   ├── path_glare.mp4            # High reflection scenario
│   └── path_mars.mp4             # Extreme domain shift
├── prompts/                       # Cosmos prompt files
│   ├── path_asphalt.json
│   ├── path_glare.json
│   └── path_mars.json
├── data/                          # Generated telemetry
│   ├── data_baseline.csv
│   ├── data_glare.csv
│   └── data_mars.csv
├── figures/                       # Generated visualizations
│   ├── comparison_video.mp4
│   ├── combined_scenario_comparison.png
│   ├── trajectory_comparison.png
│   └── [scenario]_perception_control.png
├── create_plots.py                # Individual scenario plots
├── create_additional_plots.py     # Comparison plots
├── create_comparison_video.py     # Side-by-side video generation
└── test_frame_generation.py       # Frame extraction utility
```

## Prerequisites

### Software Requirements

1. **Webots R2025a** (or compatible version)
   - Download: <https://cyberbotics.com/>
   - Installation: <https://cyberbotics.com/doc/guide/installation-procedure>

2. **Python 3.11+**
   - Tested with Python 3.11.8

### Hardware Requirements

- Multi-core CPU
- 4GB+ RAM
- GPU (optional, for video generation)

## Installation

### 1. Clone or Download Repository

```bash
cd /path/to/your/webots/project
cd controllers/vision_controller
```

### 2. Set Up Python Virtual Environment

```bash
# Create virtual environment
python3 -m venv .venv

# Activate virtual environment
# On macOS/Linux:
source .venv/bin/activate

# On Windows:
.venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install --upgrade pip
pip install opencv-python numpy pandas matplotlib
```

**Tested package versions:**

- opencv-python: 4.12.0.88
- numpy: 2.2.6
- pandas: 2.3.3
- matplotlib: 3.10.7

**Note:** The `controller` module is provided by Webots at runtime.

## Usage

### Running the Simulation

#### Step 1: Open Webots World

1. Launch Webots
2. **File → Open World**
3. Navigate to: `controllers/vision_controller/worlds/e-puck.wbt`

#### Step 2: Configure Test Scenario

Edit `vision_controller.py` (lines 8-10):

```python
# --- CONFIGURATION ---
VIDEO_PATH = "videos/path_asphalt.mp4"   # Change to test different scenarios
LOG_FILE = "data/data_baseline.csv"      # Output filename
DISPLAY_SCALE = 2.0                       # OpenCV window size
```

**Available scenarios:**

- `videos/path_asphalt.mp4` → `data/data_baseline.csv` (baseline)
- `videos/path_glare.mp4` → `data/data_glare.csv` (glare)
- `videos/path_mars.mp4` → `data/data_mars.csv` (mars)

#### Step 3: Run Controller

**Via Webots GUI:**

1. Click the robot in 3D view
2. **Robot → Edit Controller** → Select `vision_controller`
3. Click **Run** or press `Ctrl+5` (Windows/Linux) / `Cmd+5` (macOS)

**Auto-load (if configured):**

1. Click **Run** in Webots toolbar
2. Controller starts automatically

#### Step 4: Monitor Execution

- Webots 3D view shows robot motion
- OpenCV window displays video with yellow line detection (green line)
- Terminal shows data logging path
- Press `q` in OpenCV window to stop

#### Step 5: Collect Data

```bash
# View logged data
cat data/data_baseline.csv
head data/data_glare.csv
```

---

## Generating Videos with Cosmos Predict 2.5 on University of Utah CHPC

### CHPC Environment Setup

#### Container Information

**Container Locations:**

- **Primary:** `/scratch/general/vast/u1523034/cosmos/cosmos-predict25-working.sif`
- **Backup:** `/uufs/chpc.utah.edu/common/home/u1523034/containers/cosmos-predict25-working.sif`
- **Definition File:** `/uufs/chpc.utah.edu/common/home/u1523034/containers/cosmos25-working.def`

**Container Specifications:**

- **Container Size:** 11GB
- **Build Date:** December 2, 2025
- **Base Image:** nvidia/cuda:12.8.1-cudnn-devel-ubuntu24.04
- **Python:** 3.10.19
- **PyTorch:** 2.7.1+cu128
- **CUDA:** 12.8
- **Ubuntu:** 24.04

#### HuggingFace Model Cache

**Cache Configuration:**

- **Cache Location:** `/scratch/general/vast/u1523034/cosmos/hf-cache`
- **Cache Size:** ~30-40GB (auto-downloaded on first inference)
- **Environment Variable:** `HF_HOME=/scratch/general/vast/u1523034/cosmos/hf-cache`

**Cached Models:**

- nvidia/Cosmos-Predict2.5-2B (16.6GB)
- nvidia/Cosmos-Reason1.1-7B
- nvidia/Cosmos-Guardrail1
- Qwen/Qwen3Guard-Gen-0.6B

Models download automatically on first use. Subsequent runs use cached versions.

### GPU Allocation on CHPC

Request H200 GPU with SLURM:

```bash
salloc -N 1 -n 12 -A <your_account> -p <gpu_partition> \
  --qos=<gpu_qos> --gres=gpu:h200_3g.71gb:1 \
  -t 2:00:00 --mem=64G
```

Replace `<your_account>`, `<gpu_partition>`, and `<gpu_qos>` with your CHPC allocation details.

Verify GPU allocation:

```bash
nvidia-smi
```

Expected output: NVIDIA H200 with 71GB VRAM (MIG device).

### Running Inference

Load the Apptainer module:

```bash
module load apptainer
```

Run inference:

```bash
# Set environment variables
export HF_HOME=/scratch/general/vast/u1523034/cosmos/hf-cache

# Run inference
apptainer exec --nv \
  --env HF_HOME=${HF_HOME} \
  -B /scratch/general/vast/u1523034/cosmos/outputs:/outputs \
  -B ${HF_HOME}:${HF_HOME} \
  /scratch/general/vast/u1523034/cosmos/cosmos-predict25-working.sif \
  /opt/cosmos-predict2.5/.venv/bin/python \
  /opt/cosmos-predict2.5/examples/inference.py \
  -i /path/to/your/input.json \
  -o /outputs/my_scenario
```

**Command parameters:**

- `--nv`: Enable NVIDIA GPU support
- `--env HF_HOME`: Set HuggingFace cache location
- `-B`: Bind mount directories
- `-i`: Input JSON configuration file
- `-o`: Output directory name

### Creating Input Configuration

Create JSON configuration files (see `prompts/` directory):

```json
{
  "prompt": "Low-angle ground-level camera view positioned 15 centimeters above surface, moving forward at 0.5 meters per second along a bright yellow painted line on clean black asphalt pavement...",
  "num_frames": 121,
  "fps": 16,
  "height": 720,
  "width": 1280
}
```

**Parameters:**

- `prompt`: Detailed text description of desired scene
- `num_frames`: Number of frames to generate
- `fps`: Frame rate (16 recommended for Cosmos 2.5)
- `height`/`width`: Output resolution

**Prompt engineering guidelines:**

- Specify camera height, speed, and field of view
- Describe line characteristics (width, color, sharpness)
- Define environment conditions (lighting, surface texture)
- Mention motion dynamics and path geometry

### Retrieving Generated Videos

Transfer files from CHPC:

```bash
# From your local machine
scp u1523034@lonepeak.chpc.utah.edu:/scratch/general/vast/u1523034/cosmos/outputs/my_scenario/*.mp4 ./videos/
```

### Output Files

Each generation creates:

```text
/outputs/my_scenario/
├── config.yaml           # Generation parameters
├── my_scenario.json      # Input configuration
└── my_scenario.mp4       # Generated video
```

### CHPC Resources

- **CHPC Website:** <https://www.chpc.utah.edu/>
- **CHPC Documentation:** <https://www.chpc.utah.edu/documentation/>
- **CHPC Apptainer Guide:** <https://www.chpc.utah.edu/documentation/software/singularity.php>
- **CHPC Support:** <https://www.chpc.utah.edu/support/>

### Cosmos Resources

- **Official Docs:** <https://docs.nvidia.com/cosmos/latest/predict2.5/>
- **GitHub Repo:** <https://github.com/nvidia-cosmos/cosmos-predict2.5>
- **Model Card:** <https://huggingface.co/nvidia/Cosmos-Predict2.5-2B>

---

## Data Analysis and Visualization

### Generate Individual Scenario Plots

```bash
python create_plots.py
```

Output: `figures/[scenario]_perception_control.png`

4-panel visualization showing steering error, motor commands, trajectory, and error distribution.

### Generate Comparison Plots

```bash
python create_additional_plots.py
```

Output:

- `figures/combined_scenario_comparison.png`
- `figures/trajectory_comparison.png`

### Generate Side-by-Side Comparison Video

```bash
python create_comparison_video.py
```

Output: `figures/comparison_video.mp4`

1920x1080 video with 3-column layout (Baseline | Glare | Mars), synchronized playback, metrics overlay, and live steering error plot.

## Testing Methodology

### Test Scenarios

1. **Baseline (path_asphalt.mp4)**
   - Clean asphalt road
   - Optimal lighting
   - High contrast yellow line

2. **Glare (path_glare.mp4)**
   - High reflection conditions
   - Challenging lighting

3. **Mars (path_mars.mp4)**
   - Extreme domain shift
   - Mars-like terrain texture

### Evaluation Metrics

**Steering Error:**

- Range: [-1.0, +1.0] (normalized)
- Negative: line left of center
- Positive: line right of center

**Motor Commands:**

- Base speed: 3.0 rad/s
- Turn command range: ±2.5 rad/s

**Trajectory Analysis:**

- Odometry-based path reconstruction
- Comparison across scenarios

## Results

### Individual Scenario Analysis

The following figures show the perception and control performance for each test scenario:

**Baseline Scenario:**

![Baseline Perception Control](figures/baseline_perception_control.png)

The baseline scenario demonstrates optimal performance with clean asphalt and good lighting conditions.

**Glare Scenario:**

![Glare Perception Control](figures/glare_perception_control.png)

The glare scenario shows increased steering error variance due to challenging lighting and reflections.

**Mars Scenario:**

![Mars Perception Control](figures/mars_perception_control.png)

The Mars scenario exhibits the highest error variance, demonstrating the limits of perception under extreme domain shift.

### Cross-Scenario Comparison

**Combined Scenario Comparison:**

![Combined Scenario Comparison](figures/combined_scenario_comparison.png)

This figure overlays all three scenarios, showing how perception degrades from baseline to extreme conditions.

**Trajectory Comparison:**

![Trajectory Comparison](figures/trajectory_comparison.png)

Odometry-based trajectory reconstruction comparing robot paths across all scenarios.

### Video Demonstration

A side-by-side comparison video shows synchronized playback of all three scenarios with real-time metrics:

![Comparison Video](figures/comparison_video.mp4)

The video demonstrates:

- Synchronized perception across all scenarios
- Real-time steering error tracking
- Motor command response
- Yellow line detection visualization (green = detected line, red dashed = center reference)

## Implementation Notes

### Video Integration

This implementation uses `cv2.VideoCapture()` to read pre-recorded videos instead of Webots `Camera` devices. This allows testing with GenAI-generated videos without modifying the simulation world.

### Error Handling

- **Video end:** Calls `close_and_exit()` for clean shutdown
- **Line not detected:** Maintains last steering command (error = 0.0)
- **Manual exit:** Press `q` key in OpenCV window

### Critical Constraint

Standard Webots `Robot` controllers cannot pause/control the simulator (only `Supervisor` nodes can). Avoid `simulationSetMode()` calls.

---

**Last Updated:** December 2025
**Webots Version:** R2025a
**Python Version:** 3.11.8
